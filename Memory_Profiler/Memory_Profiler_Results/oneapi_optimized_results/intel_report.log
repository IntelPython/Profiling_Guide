Filename: run_benchmarks.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    62 274.6289 MiB 274.6289 MiB           1       @profile(precision=4,stream = fp)
    63                                             def get_data(path_to_csv: str) -> pd.DataFrame:
    64                                                 """Read in and clean data
    65                                         
    66                                                 Args:
    67                                                     path_to_csv (str): processed data
    68                                                 """
    69 283.9883 MiB   9.3594 MiB           1           data = pd.read_csv(path_to_csv)[
    70 386.5820 MiB 102.5938 MiB           1               ['category', 'headline', 'short_description', 'link']
    71                                                 ]
    72 382.9609 MiB  -3.6211 MiB           1           data = data.dropna(subset=['headline', 'short_description', 'link'])
    73                                         
    74 382.9961 MiB   0.0352 MiB           1           data.link = data.link.apply(clean_link)
    75 382.9961 MiB   0.0000 MiB           1           data.short_description = data.short_description \
    76 383.9805 MiB   0.9844 MiB           1               .apply(clean_short_description)
    77 386.0508 MiB   2.0703 MiB           1           data.headline = data.headline.apply(clean_headline)
    78                                         
    79                                                 data['text'] = data.link + " " + data.short_description \
    80 649.3086 MiB 263.2578 MiB           1               + " " + data.headline
    81 649.5508 MiB   0.2422 MiB           1           data['tokens'] = data.text.apply(tokenize)
    82 649.5508 MiB   0.0000 MiB           1           return data


Filename: run_benchmarks.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    62 649.5508 MiB 649.5508 MiB           1       @profile(precision=4,stream = fp)
    63                                             def get_data(path_to_csv: str) -> pd.DataFrame:
    64                                                 """Read in and clean data
    65                                         
    66                                                 Args:
    67                                                     path_to_csv (str): processed data
    68                                                 """
    69 651.0391 MiB   1.4883 MiB           1           data = pd.read_csv(path_to_csv)[
    70 655.7305 MiB   4.6914 MiB           1               ['category', 'headline', 'short_description', 'link']
    71                                                 ]
    72 655.7461 MiB   0.0156 MiB           1           data = data.dropna(subset=['headline', 'short_description', 'link'])
    73                                         
    74 655.8242 MiB   0.0781 MiB           1           data.link = data.link.apply(clean_link)
    75 655.8242 MiB   0.0000 MiB           1           data.short_description = data.short_description \
    76 655.8281 MiB   0.0039 MiB           1               .apply(clean_short_description)
    77 655.8398 MiB   0.0117 MiB           1           data.headline = data.headline.apply(clean_headline)
    78                                         
    79                                                 data['text'] = data.link + " " + data.short_description \
    80 657.1211 MiB   1.2812 MiB           1               + " " + data.headline
    81 657.1211 MiB   0.0000 MiB           1           data['tokens'] = data.text.apply(tokenize)
    82 657.1211 MiB   0.0000 MiB           1           return data


Filename: run_benchmarks.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    84 657.1211 MiB 657.1211 MiB           1       @profile(precision=4,stream = fp)
    85                                             def train_data(train,test):
    86 657.1211 MiB   0.0000 MiB           1           vectorizer = TfidfVectorizer(
    87 657.1211 MiB   0.0000 MiB           1               min_df=50,
    88 657.1211 MiB   0.0000 MiB           1               lowercase=False,
    89 3492.2344 MiB 703.0547 MiB      362281               tokenizer=lambda x: x)
    90                                         
    91 657.1211 MiB   0.0000 MiB           1           svc = SVC()
    92 3626.7305 MiB 2132.0586 MiB           1           svc.fit(vectorizer.fit_transform(train.tokens), train.category)
    93                                         
    94 3626.7305 MiB   0.0000 MiB           1           training_time = time.time()
    95                                         
    96                                                 # Predict on unseen test data
    97 4836.2734 MiB 1344.0391 MiB           1           y_pred = svc.predict(vectorizer.transform(test.tokens))
    98 4836.2734 MiB   0.0000 MiB           1           return svc, training_time, y_pred


