Filename: run_benchmarks.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    62 125.4531 MiB 125.4531 MiB           1       @profile(precision=4,stream = fp)
    63                                             def get_data(path_to_csv: str) -> pd.DataFrame:
    64                                                 """Read in and clean data
    65                                         
    66                                                 Args:
    67                                                     path_to_csv (str): processed data
    68                                                 """
    69 226.3516 MiB 100.8984 MiB           1           data = pd.read_csv(path_to_csv)[
    70 217.8242 MiB  -8.5273 MiB           1               ['category', 'headline', 'short_description', 'link']
    71                                                 ]
    72 227.6562 MiB   9.8320 MiB           1           data = data.dropna(subset=['headline', 'short_description', 'link'])
    73                                         
    74 236.3984 MiB   8.7422 MiB           1           data.link = data.link.apply(clean_link)
    75 236.3984 MiB   0.0000 MiB           1           data.short_description = data.short_description \
    76 218.0273 MiB -18.3711 MiB           1               .apply(clean_short_description)
    77 228.6602 MiB  10.6328 MiB           1           data.headline = data.headline.apply(clean_headline)
    78                                         
    79                                                 data['text'] = data.link + " " + data.short_description \
    80 272.7461 MiB  44.0859 MiB           1               + " " + data.headline
    81 796.3984 MiB 523.6523 MiB           1           data['tokens'] = data.text.apply(tokenize)
    82 796.3984 MiB   0.0000 MiB           1           return data


Filename: run_benchmarks.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    62 796.3984 MiB 796.3984 MiB           1       @profile(precision=4,stream = fp)
    63                                             def get_data(path_to_csv: str) -> pd.DataFrame:
    64                                                 """Read in and clean data
    65                                         
    66                                                 Args:
    67                                                     path_to_csv (str): processed data
    68                                                 """
    69 805.9023 MiB   9.5039 MiB           1           data = pd.read_csv(path_to_csv)[
    70 805.9023 MiB   0.0000 MiB           1               ['category', 'headline', 'short_description', 'link']
    71                                                 ]
    72 805.9023 MiB   0.0000 MiB           1           data = data.dropna(subset=['headline', 'short_description', 'link'])
    73                                         
    74 806.2734 MiB   0.3711 MiB           1           data.link = data.link.apply(clean_link)
    75 806.2734 MiB   0.0000 MiB           1           data.short_description = data.short_description \
    76 804.7266 MiB  -1.5469 MiB           1               .apply(clean_short_description)
    77 804.7266 MiB   0.0000 MiB           1           data.headline = data.headline.apply(clean_headline)
    78                                         
    79                                                 data['text'] = data.link + " " + data.short_description \
    80 812.7031 MiB   7.9766 MiB           1               + " " + data.headline
    81 904.0234 MiB  91.3203 MiB           1           data['tokens'] = data.text.apply(tokenize)
    82 904.0234 MiB   0.0000 MiB           1           return data


Filename: run_benchmarks.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    84 904.0234 MiB 904.0234 MiB           1       @profile(precision=4,stream = fp)
    85                                             def train_data(train,test):
    86 904.0234 MiB   0.0000 MiB           1           vectorizer = TfidfVectorizer(
    87 904.0234 MiB   0.0000 MiB           1               min_df=50,
    88 904.0234 MiB   0.0000 MiB           1               lowercase=False,
    89 1487.9609 MiB 224.4180 MiB      362281               tokenizer=lambda x: x)
    90                                         
    91 904.0234 MiB   0.0000 MiB           1           svc = SVC()
    92 1487.2812 MiB 359.5195 MiB           1           svc.fit(vectorizer.fit_transform(train.tokens), train.category)
    93                                         
    94 1487.2812 MiB   0.0000 MiB           1           training_time = time.time()
    95                                         
    96                                                 # Predict on unseen test data
    97 1487.7109 MiB  -0.2500 MiB           1           y_pred = svc.predict(vectorizer.transform(test.tokens))
    98 1487.7109 MiB   0.0000 MiB           1           return svc, training_time, y_pred


