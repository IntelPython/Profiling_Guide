{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6564ad7-2463-4f7d-a9e1-3dcb6d9b666c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Line Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf283e-b70a-43be-a454-00fb75f0214c",
   "metadata": {},
   "source": [
    "Function profiling tools only time function calls. This is a good first step for locating hotspots in one's program and is frequently all one needs to do to optimize the program. **However, sometimes the cause of the hotspot is actually a single line in the function, and that line may not be obvious from just reading the source code**. These cases are particularly frequent in scientific computing. Functions tend to be larger (sometimes because of legitimate algorithmic complexity, sometimes because the programmer is still trying to write FORTRAN code), and a single statement without function calls can trigger lots of computation when using libraries like numpy. cProfile only times explicit function calls, not special methods called because of syntax. Consequently, a relatively slow numpy operation on large arrays like this,\n",
    "```\n",
    "a[large_index_array] = some_other_large_array\n",
    "```\n",
    "is a hotspot that never gets broken out by cProfile because there is no explicit function call in that statement.\n",
    "\n",
    "***LineProfiler can be given functions to profile, and it will time the execution of each individual line inside those functions.*** <br>\n",
    "\n",
    "To know more about line profiler visit [link](https://github.com/pyutils/line_profiler) <br>\n",
    "\n",
    "**To Install line_profiler** <br>\n",
    "**Using Conda**\n",
    "```\n",
    "conda install line_profiler\n",
    "```\n",
    "**Using pip**\n",
    "```\n",
    "$ pip install line_profiler\n",
    "\n",
    "To install compatible IPython version using pip:\n",
    "$ pip install line_profiler[ipython]\n",
    "\n",
    "To check out the development sources, you can use Git:\n",
    "$ git clone https://github.com/pyutils/line_profiler.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09af3c0-bfc5-4575-8521-9c9df335ce96",
   "metadata": {},
   "source": [
    "Ways to Profile Python Code using \"line_profiler\"\n",
    "- **kernprof**: Command Prompt/Shell Command: This command let us profile whole Python script from command line/shell.\n",
    "- **LineProfiler**: Object in Python Script: This let us profile individual functions of our code by declaring profiler object in script itself.\n",
    "- **%lprun**: Jupyter Notebook Magic Command: This let us profile functions in Jupyter Notebooks using \"%lprun\" line magic command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f8c06-62fc-4f84-904a-fda323a54f03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example to use KernProf and Line_Profiler Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eeb993-bdc5-49aa-bf18-ce61faf9ea09",
   "metadata": {},
   "source": [
    "Create a python file **random_number_average.py** with following code. We have already created one for your reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a27379f-915f-4a6d-9991-552a42e9a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import random\n",
    "\n",
    "# @profile\n",
    "# def very_slow_random_generator():\n",
    "#     time.sleep(5)\n",
    "#     arr = [random.randint(1,100) for i in range(100000)]\n",
    "#     return sum(arr) / len(arr)\n",
    "\n",
    "# @profile\n",
    "# def slow_random_generator():\n",
    "#     time.sleep(2)\n",
    "#     arr = [random.randint(1,100) for i in range(100000)]\n",
    "#     return sum(arr) / len(arr)\n",
    "    \n",
    "# @profile\n",
    "# def main_func():\n",
    "#     result = slow_random_generator()\n",
    "#     print(result)\n",
    "\n",
    "#     result = very_slow_random_generator()\n",
    "#     print(result)\n",
    "\n",
    "# main_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157d66e-3cfc-432b-b6f9-158d8560237f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### KernProf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf005436-108d-486a-8432-6c654962ede4",
   "metadata": {},
   "source": [
    "To Profile this file use decorators **@profile** above the function use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9bc1b9-43bf-476a-ba36-2967fa547abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.61365\n",
      "50.55554\n",
      "Wrote profile results to random_number_average.py.lprof\n"
     ]
    }
   ],
   "source": [
    "!kernprof -l random_number_average.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e794d2-a042-4cdd-97fc-d6550fbfdee0",
   "metadata": {},
   "source": [
    "To see **output** from the above file, use the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40303d53-1a0b-415d-b9b0-4db1130ed3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 5.28899 s\n",
      "File: random_number_average.py\n",
      "Function: very_slow_random_generator at line 4\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     4                                           @profile\n",
      "     5                                           def very_slow_random_generator():\n",
      "     6         1    5002783.5 5002783.5     94.6      time.sleep(5)\n",
      "     7         1     285592.4 285592.4      5.4      arr = [random.randint(1,100) for i in range(100000)]\n",
      "     8         1        613.6    613.6      0.0      return sum(arr) / len(arr)\n",
      "\n",
      "Total time: 2.28891 s\n",
      "File: random_number_average.py\n",
      "Function: slow_random_generator at line 10\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    10                                           @profile\n",
      "    11                                           def slow_random_generator():\n",
      "    12         1    2002051.3 2002051.3     87.5      time.sleep(2)\n",
      "    13         1     286242.2 286242.2     12.5      arr = [random.randint(1,100) for i in range(100000)]\n",
      "    14         1        615.0    615.0      0.0      return sum(arr) / len(arr)\n",
      "\n",
      "Total time: 7.57832 s\n",
      "File: random_number_average.py\n",
      "Function: main_func at line 16\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    16                                           @profile\n",
      "    17                                           def main_func():\n",
      "    18         1    2289093.1 2289093.1     30.2      result = slow_random_generator()\n",
      "    19         1         44.8     44.8      0.0      print(result)\n",
      "    20                                           \n",
      "    21         1    5289144.5 5289144.5     69.8      result = very_slow_random_generator()\n",
      "    22         1         36.1     36.1      0.0      print(result)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m line_profiler random_number_average.py.lprof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4bcd56-19b4-41a0-8d82-ffd69efcdfc7",
   "metadata": {},
   "source": [
    "To understand the results above, the different column definitions are given below\n",
    "- **Hits:** The first column represents number of times that line was hit inside that function. In our example hits it is 1 but it can be more than one in case of recurrences.\n",
    "- **Time:** The second column represents the time taken by that line in total for all hits. This time is in microseconds.\n",
    "- **Per Hit:** The third column represents time taken per each call of that line.\n",
    "- **% Time:** The fourth column represents % of time taken by that line of total function time.\n",
    "- **Line Contents:** The fifth column represents code in that line of function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d636de3-1078-4b56-9e8d-701126526111",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Line Profiler Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea84f4fc-7df9-4d38-94e4-42932810d834",
   "metadata": {},
   "source": [
    "We then need to create an **object of LineProfiler class** first. We then need to create a wrapper around main_func() by calling the LineProfiler instance passing it main_func. We can then execute that line profiler wrapper which will execute main_func()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1947dde-13f0-495f-be53-92ffca6318bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Add this code in your python file and to add multiple functions use **add_function**\n",
    "```\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "lprofiler = LineProfiler()\n",
    "\n",
    "lprofiler.add_function(very_slow_random_generator)\n",
    "lprofiler.add_function(slow_random_generator)\n",
    "\n",
    "lp_wrapper = lprofiler(main_func)\n",
    "\n",
    "lp_wrapper()\n",
    "\n",
    "lprofiler.print_stats()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc58a90-ba7e-4aa8-854c-4d5d8f313a0c",
   "metadata": {},
   "source": [
    "We have an example file with name **random_number_average_with_lp_object.py**. To view the results run the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e419bea-dcf4-445e-97be-4a0c19fe86e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.5467\n",
      "50.55348\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 5.29262 s\n",
      "File: random_number_average_with_lp_object.py\n",
      "Function: very_slow_random_generator at line 6\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     6                                           def very_slow_random_generator():\n",
      "     7         1 5005053490.0 5005053490.0     94.6      time.sleep(5)\n",
      "     8         1  286959649.0 286959649.0      5.4      arr = [random.randint(1,100) for i in range(100000)]\n",
      "     9         1     611228.0 611228.0      0.0      return sum(arr) / len(arr)\n",
      "\n",
      "Total time: 2.29 s\n",
      "File: random_number_average_with_lp_object.py\n",
      "Function: slow_random_generator at line 11\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    11                                           def slow_random_generator():\n",
      "    12         1 2002050071.0 2002050071.0     87.4      time.sleep(2)\n",
      "    13         1  287334481.0 287334481.0     12.5      arr = [random.randint(1,100) for i in range(100000)]\n",
      "    14         1     616737.0 616737.0      0.0      return sum(arr) / len(arr)\n",
      "\n",
      "Total time: 7.58301 s\n",
      "File: random_number_average_with_lp_object.py\n",
      "Function: main_func at line 16\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    16                                           def main_func():\n",
      "    17         1 2290177405.0 2290177405.0     30.2      result = slow_random_generator()\n",
      "    18         1      39018.0  39018.0      0.0      print(result)\n",
      "    19                                           \n",
      "    20         1 5292761862.0 5292761862.0     69.8      result = very_slow_random_generator()\n",
      "    21         1      33444.0  33444.0      0.0      print(result)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python random_number_average_with_lp_object.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3de7e-a238-4ea4-a308-8dc2bdd643e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Profile [Intelligent Indexing](https://github.com/oneapi-src/intelligent-indexing) Ref kit using %lprun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0020a2-34f8-43d4-b9f5-0c174eecc4c7",
   "metadata": {},
   "source": [
    "The **[Intelligent Indexing](https://github.com/oneapi-src/intelligent-indexing)** ref kit demonstrates one way of building an NLP pipeline for classifying documents to their respective topics and describe how we can leverage the **Intel® oneAPI AI Analytics Toolkit (oneAPI)** to accelerate the pipeline.\n",
    "\n",
    "**Intel® oneAPI** is used to achieve quick results even when the data for a model are huge. It provides the capability to reuse the code present in different languages so that the hardware utilization is optimized to provide these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b545dd0-f3f2-409a-a018-ed04afdbcbc7",
   "metadata": {},
   "source": [
    "The **Intelligent Indexing** ref kit has different Intel® oneAPI optimizations enabled like:\n",
    "- **[Intel® oneAPI Modin](https://www.intel.com/content/www/us/en/developer/tools/oneapi/distribution-of-modin.html#gs.v03x2l)**\n",
    "The Intel® Distribution of Modin* is a performant, parallel, and distributed dataframe system that is designed around enabling data scientists to be more productive. It provides drop-in acceleration to your existing **pandas** workflows. No upfront cost to learning a new API. Integrates with the Python* ecosystem. Seamlessly scales across multicores with Ray* and Dask* clusters (run on and with what you have)\n",
    "- **[Intel® oneAPI Scikit-Learn-Extension](https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html)**\n",
    "Designed for data scientists, Intel® Extension for Scikit-Learn* is a seamless way to speed up your Scikit-learn applications for machine learning to solve real-world problems. This extension package dynamically patches scikit-learn estimators to use Intel® oneAPI Data Analytics Library (oneDAL) as the underlying solver, while achieving the speed up for your machine learning algorithms out-of-box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41287718-310e-40ad-b29f-53915cfffc00",
   "metadata": {},
   "source": [
    "**NOTE** Please visit the **[Intelligent Indexing](https://github.com/oneapi-src/intelligent-indexing)** Ref kit page to know more about the kit.\n",
    "- Please follow the steps in github repo to clone and create the environment.\n",
    "- After creating environment install **line_profiler** in both the environments **doc_class_stock** and **doc_class_intel** using\n",
    "```\n",
    "conda install line_profiler\n",
    "```\n",
    "We will be using **line_profiler** to profile this workload below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f3ae4-09d3-41b4-97a7-901c0c3527f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### %lprun Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3042c3f-caaf-4d27-9bd7-973ee42cd35a",
   "metadata": {},
   "source": [
    "- Load Line Profiler using load_ext line_profiler\n",
    "- lprun? : To get help for usage of lprun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25685bc6-1225-4e35-87a7-2720ebf43d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b535e36-ac33-4b33-b2b3-dbe6f4c17772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Execute a statement under the line-by-line profiler from the\n",
       "line_profiler module.\n",
       "\n",
       "Usage:\n",
       "%lprun -f func1 -f func2 <statement>\n",
       "\n",
       "The given statement (which doesn't require quote marks) is run via the\n",
       "LineProfiler. Profiling is enabled for the functions specified by the -f\n",
       "options. The statistics will be shown side-by-side with the code through the\n",
       "pager once the statement has completed.\n",
       "\n",
       "Options:\n",
       "\n",
       "-f <function>: LineProfiler only profiles functions and methods it is told\n",
       "to profile.  This option tells the profiler about these functions. Multiple\n",
       "-f options may be used. The argument may be any expression that gives\n",
       "a Python function or method object. However, one must be careful to avoid\n",
       "spaces that may confuse the option parser.\n",
       "\n",
       "-m <module>: Get all the functions/methods in a module\n",
       "\n",
       "One or more -f or -m options are required to get any useful results.\n",
       "\n",
       "-D <filename>: dump the raw statistics out to a pickle file on disk. The\n",
       "usual extension for this is \".lprof\". These statistics may be viewed later\n",
       "by running line_profiler.py as a script.\n",
       "\n",
       "-T <filename>: dump the text-formatted statistics with the code side-by-side\n",
       "out to a text file.\n",
       "\n",
       "-r: return the LineProfiler object after it has completed profiling.\n",
       "\n",
       "-s: strip out all entries from the print-out that have zeros.\n",
       "\n",
       "-u: specify time unit for the print-out in seconds.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/doc_class_intel/lib/python3.7/site-packages/line_profiler/ipython_extension.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc99e1ae-b237-433b-b154-7909c5582581",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Profile Intelligent Indexing Ref Kit with Stock packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71871d1e-0ae9-44cf-a28d-73f973db4f86",
   "metadata": {},
   "source": [
    "Create an object as shown below to pass parameters to the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cfbbf1-e888-4822-a6a5-19914e90ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# mkdir -p ./Line_profile_results  # create `Line_profile_results` dir in the parent dir if not present\n",
    "# mkdir -p ./Line_profile_results/stock_results  # create `stock_results` dir in the Line_profile_results if not present\n",
    "# mkdir -p ./Line_profile_results/ipex_results  # create `ipex_results` dir in the Line_profile_results if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702d2c0-28ee-4239-84f6-572811e1e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_benchmarks import main\n",
    "C = type('C', (object,), {})\n",
    "d = C()\n",
    "d.intel = False\n",
    "d.logfile = '../intelligent-indexing/logs/stock_stock.log'\n",
    "d.preprocessing_only = False\n",
    "d.save_model_dir = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f799d7b-094a-4190-93bc-6defd156bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'lprof0'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 11554.1 s\n",
       "File: /ws2/yfulwani/intelligent-indexing/src/run_benchmarks.py\n",
       "Function: main at line 32\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    32                                               \"\"\"Setup model for inference and perform benchmarking\n",
       "    33                                           \n",
       "    34                                               Args:\n",
       "    35                                                   flags: benchmarking flags\n",
       "    36                                               \"\"\"\n",
       "    37                                           \n",
       "    38                                               if flags.logfile == \"\":\n",
       "    39         1       1766.0   1766.0      0.0          logging.basicConfig(level=logging.DEBUG)\n",
       "    40                                               else:\n",
       "    41                                                   logging.basicConfig(filename=flags.logfile, level=logging.DEBUG)\n",
       "    42         1     242081.0 242081.0      0.0      logger = logging.getLogger()\n",
       "    43         1       2369.0   2369.0      0.0  \n",
       "    44                                               if flags.intel:\n",
       "    45         1        549.0    549.0      0.0          logging.debug(\"Loading intel libraries...\")\n",
       "    46                                           \n",
       "    47                                                   import modin.pandas as pd\n",
       "    48                                                   from sklearnex.svm import SVC\n",
       "    49                                                   import ray\n",
       "    50                                                   ray.init()\n",
       "    51                                           \n",
       "    52                                               else:\n",
       "    53                                                   logging.debug(\"Loading stock libraries...\")\n",
       "    54         1     208691.0 208691.0      0.0  \n",
       "    55                                                   import pandas as pd\n",
       "    56         1  258531370.0 258531370.0      0.0          from sklearn.svm import SVC\n",
       "    57         1       5409.0   5409.0      0.0  \n",
       "    58                                               def get_data(path_to_csv: str) -> pd.DataFrame:\n",
       "    59         1       1955.0   1955.0      0.0          \"\"\"Read in and clean data\n",
       "    60                                           \n",
       "    61                                                   Args:\n",
       "    62                                                       path_to_csv (str): processed data\n",
       "    63                                                   \"\"\"\n",
       "    64                                                   data = pd.read_csv(path_to_csv)[\n",
       "    65                                                       ['category', 'headline', 'short_description', 'link']\n",
       "    66                                                   ]\n",
       "    67                                                   data = data.dropna(subset=['headline', 'short_description', 'link'])\n",
       "    68                                           \n",
       "    69                                                   data.link = data.link.apply(clean_link)\n",
       "    70                                                   data.short_description = data.short_description \\\n",
       "    71                                                       .apply(clean_short_description)\n",
       "    72                                                   data.headline = data.headline.apply(clean_headline)\n",
       "    73                                           \n",
       "    74                                                   data['text'] = data.link + \" \" + data.short_description \\\n",
       "    75                                                       + \" \" + data.headline\n",
       "    76                                                   data['tokens'] = data.text.apply(tokenize)\n",
       "    77                                                   return data"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -T ./Line_profile_results/stock_results/line_stock.txt -f main main(d) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a36db-8fbe-4dc7-beac-fd0d61c4642a",
   "metadata": {},
   "source": [
    "You can also save the profiling stats using flag **-D** as fname.prof and then use **snakeviz** to visualize the profiling results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df83f8-e438-459e-8c50-d3639a274b65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Profile Intelligent Indexing Ref Kit with Intel oneAPI optimized packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8838994-e1dd-463f-bc37-79dee7847f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = type('C', (object,), {})\n",
    "d = C()\n",
    "d.intel = True\n",
    "d.logfile = '../intelligent-indexing/logs/intel_intel.log'\n",
    "d.preprocessing_only = False\n",
    "d.save_model_dir = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2891fa-3462-4021-8c19-4c745b7c7872",
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -T ./Line_profile_results/oneapi_optmized_results/line_oneapi_optmized.txt -f main main(d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e939f6ba-529b-4fda-b801-cfbc238bae0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 114.579 s\n",
      "File: /ws2/yfulwani/intelligent-indexing/src/run_benchmarks.py\n",
      "Function: main at line 32\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    32                                           def main(flags):\n",
      "    33                                               \"\"\"Setup model for inference and perform benchmarking\n",
      "    34                                           \n",
      "    35                                               Args:\n",
      "    36                                                   flags: benchmarking flags\n",
      "    37                                               \"\"\"\n",
      "    38                                           \n",
      "    39         1       3493.0   3493.0      0.0      if flags.logfile == \"\":\n",
      "    40                                                   logging.basicConfig(level=logging.DEBUG)\n",
      "    41                                               else:\n",
      "    42         1    1561933.0 1561933.0      0.0          logging.basicConfig(filename=flags.logfile, level=logging.DEBUG)\n",
      "    43         1       5000.0   5000.0      0.0      logger = logging.getLogger()\n",
      "    44                                           \n",
      "    45         1       1060.0   1060.0      0.0      if flags.intel:\n",
      "    46         1     380165.0 380165.0      0.0          logging.debug(\"Loading intel libraries...\")\n",
      "    47                                           \n",
      "    48         1  546212174.0 546212174.0      0.5          import modin.pandas as pd\n",
      "    49         1  313110605.0 313110605.0      0.3          from sklearnex.svm import SVC\n",
      "    50         1  293736186.0 293736186.0      0.3          import ray\n",
      "    51         1 3145692321.0 3145692321.0      2.7          ray.init()\n",
      "    52                                           \n",
      "    53                                               else:\n",
      "    54                                                   logging.debug(\"Loading stock libraries...\")\n",
      "    55                                           \n",
      "    56                                                   import pandas as pd\n",
      "    57                                                   from sklearn.svm import SVC\n",
      "    58                                           \n",
      "    59         1     178578.0 178578.0      0.0      def get_data(path_to_csv: str) -> pd.DataFrame:\n",
      "    60                                                   \"\"\"Read in and clean data\n",
      "    61                                           \n",
      "    62                                                   Args:\n",
      "    63                                                       path_to_csv (str): processed data\n",
      "    64                                                   \"\"\"\n",
      "    65                                                   data = pd.read_csv(path_to_csv)[\n",
      "    66                                                       ['category', 'headline', 'short_description', 'link']\n",
      "    67                                                   ]\n",
      "    68                                                   data = data.dropna(subset=['headline', 'short_description', 'link'])\n",
      "    69                                           \n",
      "    70                                                   data.link = data.link.apply(clean_link)\n",
      "    71                                                   data.short_description = data.short_description \\\n",
      "    72                                                       .apply(clean_short_description)\n",
      "    73                                                   data.headline = data.headline.apply(clean_headline)\n",
      "    74                                           \n",
      "    75                                                   data['text'] = data.link + \" \" + data.short_description \\\n",
      "    76                                                       + \" \" + data.headline\n",
      "    77                                                   data['tokens'] = data.text.apply(tokenize)\n",
      "    78                                                   return data\n",
      "    79                                           \n",
      "    80                                               # Read and clean training and testing data\n",
      "    81         1    1387967.0 1387967.0      0.0      logger.info(\"Preprocessing Data...\")\n",
      "    82         1       3320.0   3320.0      0.0      start = time.time()\n",
      "    83                                           \n",
      "    84         1      51320.0  51320.0      0.0      if not os.path.exists(\"../data/huffpost/train_all.csv\"):\n",
      "    85                                                   logger.error(\n",
      "    86                                                       \"Train data file ../data/huffpost/train_all.csv not found\")\n",
      "    87                                                   return\n",
      "    88                                           \n",
      "    89         1      16527.0  16527.0      0.0      if not os.path.exists(\"../data/huffpost/test.csv\"):\n",
      "    90                                                   logger.error(\"Test data file ../data/huffpost/test.csv not found\")\n",
      "    91                                                   return\n",
      "    92                                           \n",
      "    93         1 12158574276.0 12158574276.0     10.6      train = get_data(\"../data/huffpost/train_all.csv\")\n",
      "    94         1 10549726145.0 10549726145.0      9.2      test = get_data(\"../data/huffpost/test.csv\")\n",
      "    95                                           \n",
      "    96         1       6068.0   6068.0      0.0      preprocessing_time = time.time()\n",
      "    97         1      12557.0  12557.0      0.0      logger.info(\n",
      "    98         1       2840.0   2840.0      0.0          \"=======> Preprocessing Time : %.3f secs\",\n",
      "    99         1    1596988.0 1596988.0      0.0          preprocessing_time - start\n",
      "   100                                               )\n",
      "   101                                           \n",
      "   102         1      11262.0  11262.0      0.0      if not flags.preprocessing_only:\n",
      "   103                                           \n",
      "   104                                                   # Build TFIDF features and train the model\n",
      "   105         1     192601.0 192601.0      0.0          logger.debug(\"Training & Evaluating Model...\")\n",
      "   106                                           \n",
      "   107         1       3534.0   3534.0      0.0          vectorizer = TfidfVectorizer(\n",
      "   108         1        557.0    557.0      0.0              min_df=50,\n",
      "   109         1        441.0    441.0      0.0              lowercase=False,\n",
      "   110         1     203995.0 203995.0      0.0              tokenizer=lambda x: x)\n",
      "   111                                           \n",
      "   112         1     204357.0 204357.0      0.0          svc = SVC()\n",
      "   113         1 59026240848.0 59026240848.0     51.5          svc.fit(vectorizer.fit_transform(train.tokens), train.category)\n",
      "   114                                           \n",
      "   115         1       4642.0   4642.0      0.0          training_time = time.time()\n",
      "   116                                           \n",
      "   117                                                   # Predict on unseen test data\n",
      "   118         1 28201801811.0 28201801811.0     24.6          y_pred = svc.predict(vectorizer.transform(test.tokens))\n",
      "   119                                           \n",
      "   120         1       7579.0   7579.0      0.0          prediction_time = time.time()\n",
      "   121                                           \n",
      "   122         1      14231.0  14231.0      0.0          if flags.save_model_dir:\n",
      "   123                                           \n",
      "   124                                                       path = pathlib.Path(flags.save_model_dir)\n",
      "   125                                                       path.mkdir(parents=True, exist_ok=True)\n",
      "   126                                           \n",
      "   127                                                       with open(path / \"model.pkl\", 'wb') as outfile:\n",
      "   128                                                           joblib.dump(svc, outfile)\n",
      "   129                                           \n",
      "   130         1      11028.0  11028.0      0.0          logger.info(\n",
      "   131         1       3196.0   3196.0      0.0              \"=======> Test Accuracy : %.2f\",\n",
      "   132         1  337484977.0 337484977.0      0.3              accuracy_score(y_pred, test.category)\n",
      "   133                                                   )\n",
      "   134         1        379.0    379.0      0.0          logger.info(\n",
      "   135         1        598.0    598.0      0.0              \"=======> Training Time : %.3f secs\",\n",
      "   136         1      58500.0  58500.0      0.0              training_time - preprocessing_time\n",
      "   137                                                   )\n",
      "   138         1        273.0    273.0      0.0          logger.info(\n",
      "   139         1        201.0    201.0      0.0              \"=======> Inference Time : %.3f secs\",\n",
      "   140         1      33440.0  33440.0      0.0              prediction_time - training_time\n",
      "   141                                                   )\n",
      "   142         1        238.0    238.0      0.0          logger.info(\n",
      "   143         1        207.0    207.0      0.0              \"=======> Total time : %.3f secs\",\n",
      "   144         1      32956.0  32956.0      0.0              prediction_time - start\n",
      "   145                                                   )\n"
     ]
    }
   ],
   "source": [
    "print(open('./Line_profile_results/oneapi_optmized_results/line_oneapi_optmized.txt', 'r').read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
